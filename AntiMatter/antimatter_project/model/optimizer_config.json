{
    "optimizer": "AdamW",
    "learning_rate": 3e-4,
    "betas": [
        0.9,
        0.95
    ],
    "weight_decay": 0.1,
    "epsilon": 1e-8,
    "scheduler": {
        "type": "cosine",
        "warmup_steps": 2000,
        "max_steps": 100000,
        "min_lr_ratio": 0.1
    },
    "gradient_clipping": 1.0,
    "fp16": true
}